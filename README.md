# ğŸ¤– QA Local - Chatbot RAG Local

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-green.svg)](https://fastapi.tiangolo.com)
[![LangChain](https://img.shields.io/badge/LangChain-0.1%2B-orange.svg)](https://langchain.com)
[![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-purple.svg)](https://ollama.ai)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Un systÃ¨me de questions-rÃ©ponses intelligent basÃ© sur **RAG** (Retrieval-Augmented Generation) qui utilise des modÃ¨les locaux avec Ollama. Interrogez vos documents PDF, TXT et Markdown de maniÃ¨re conversationnelle, sans dÃ©pendre de services cloud externes.

![QA Local Demo](https://via.placeholder.com/800x400/2c3e50/ffffff?text=QA+Local+Chatbot+Demo)

## ğŸŒŸ FonctionnalitÃ©s

- ğŸ” **RAG Local** - Recherche intelligente dans vos documents
- ğŸ“š **Multi-formats** - Support PDF, TXT, Markdown
- ğŸ–¥ï¸ **Interface CLI** - Terminal interactif avec Rich UI
- ğŸŒ **API REST** - Endpoints FastAPI pour intÃ©gration
- ğŸ§  **Ollama Integration** - ModÃ¨les LLM locaux (Mistral, Llama, etc.)
- ğŸ’¾ **Base vectorielle** - Stockage persistant avec Chroma
- ğŸ¯ **Recherche prÃ©cise** - RÃ©cupÃ©ration de documents pertinents
- ğŸ”§ **Configurable** - Variables d'environnement flexibles

## ğŸ—ï¸ Architecture

```
qa_local/
â”œâ”€â”€ ğŸ“ data/           # Documents sources (PDF, TXT, MD)
â”œâ”€â”€ ğŸ“ index/          # Index vectoriel Chroma (gÃ©nÃ©rÃ©)
â”œâ”€â”€ ğŸ“„ api.py          # API REST FastAPI
â”œâ”€â”€ ğŸ“„ ingest.py       # Script d'ingestion des documents
â”œâ”€â”€ ğŸ“„ qa_cli.py       # Interface en ligne de commande
â”œâ”€â”€ ğŸ“„ utils.py        # Utilitaires de traitement
â””â”€â”€ ğŸ“„ README.md       # Ce fichier
```

## ğŸš€ Installation Rapide

### PrÃ©requis

- Python 3.8+
- [Ollama](https://ollama.ai) installÃ© localement

### 1. Installation d'Ollama

```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# DÃ©marrer Ollama
ollama serve
```

### 2. Installation des modÃ¨les

```bash
# ModÃ¨le d'embedding (requis)
ollama pull nomic-embed-text

# ModÃ¨le de gÃ©nÃ©ration (recommandÃ©)
ollama pull mistral:7b-instruct
```

### 3. Configuration du projet

```bash
# Cloner le projet
git clone <votre-repo>/qa_local.git
cd qa_local

# CrÃ©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“¦ DÃ©pendances

CrÃ©ez un fichier `requirements.txt` :

```txt
fastapi>=0.100.0
uvicorn[standard]>=0.20.0
langchain>=0.1.0
langchain-community>=0.0.10
langchain-ollama>=0.1.0
chromadb>=0.4.0
pypdf>=3.0.0
rich>=13.0.0
pydantic>=2.0.0
python-multipart>=0.0.6
```

## ğŸ”§ Configuration

CrÃ©ez un fichier `.env` (optionnel) :

```env
# RÃ©pertoires
DATA_DIR=data
INDEX_DIR=index

# ModÃ¨les Ollama
EMBED_MODEL=nomic-embed-text
LLM_MODEL=mistral:7b-instruct

# Configuration RAG
COLLECTION=qa_local
TOP_K=4
```

## ğŸ“š Utilisation

### 1. PrÃ©parer vos documents

Placez vos documents dans le dossier `data/` :

```bash
mkdir -p data
cp mes_documents.pdf data/
cp guide.txt data/
cp notes.md data/
```

### 2. Indexer les documents

```bash
python ingest.py
```

**Sortie attendue :**
```
ğŸš€ DÃ©but de l'ingestion des documents
âœ… 15 documents chargÃ©s  
âœ… 127 chunks crÃ©Ã©s
ğŸ‰ Indexation terminÃ©e!
ğŸ“Š 127 documents dans la collection 'qa_local'
```

### 3. Interface en ligne de commande

```bash
python qa_cli.py
```

**FonctionnalitÃ©s CLI :**
- â“ Posez vos questions naturellement
- `/stats` - Statistiques de la base
- `/help` - Aide et commandes
- `/clear` - Effacer l'Ã©cran
- `Ctrl+C` - Quitter

### 4. API REST

```bash
uvicorn api:app --reload --host 0.0.0.0 --port 8000
```

**Endpoints disponibles :**

| Endpoint | MÃ©thode | Description |
|----------|---------|-------------|
| `/qa` | POST | Question-RÃ©ponse principale |
| `/health` | GET | Ã‰tat de santÃ© de l'API |
| `/collections` | GET | Statistiques de la collection |
| `/docs` | GET | Documentation Swagger |

**Exemple d'usage :**

```bash
curl -X POST "http://localhost:8000/qa" \
  -H "Content-Type: application/json" \
  -d '{"question": "Quel est le sujet principal du document?"}'
```

**RÃ©ponse JSON :**
```json
{
  "answer": "Le document traite principalement de...",
  "sources": ["document1.pdf", "guide.txt"]
}
```

## ğŸ”„ Workflow Complet

```mermaid
graph TD
    A[Documents Sources] --> B[Ingestion]
    B --> C[DÃ©coupage en Chunks]
    C --> D[GÃ©nÃ©ration Embeddings]
    D --> E[Stockage Chroma]
    E --> F[Interface Utilisateur]
    F --> G{Type d'interface?}
    G -->|CLI| H[Terminal Rich]
    G -->|API| I[FastAPI REST]
    H --> J[Question Utilisateur]
    I --> J
    J --> K[Recherche Vectorielle]
    K --> L[GÃ©nÃ©ration RÃ©ponse LLM]
    L --> M[RÃ©ponse + Sources]
```

## ğŸ¯ Exemples d'Usage

### Questions Simples
```
â“ Qu'est-ce que l'intelligence artificielle ?
â“ RÃ©sume le contenu du rapport financier
â“ Quelles sont les Ã©tapes du processus ?
```

### Questions Complexes
```
â“ Compare les approches mentionnÃ©es dans les documents
â“ Quels sont les avantages et inconvÃ©nients listÃ©s ?
â“ Donne-moi les chiffres clÃ©s du dernier trimestre
```

## ğŸ› ï¸ Personnalisation

### Changer de modÃ¨le LLM

```bash
# Installer un nouveau modÃ¨le
ollama pull llama3:8b

# Modifier la variable d'environnement
export LLM_MODEL=llama3:8b

# Ou dans le .env
LLM_MODEL=llama3:8b
```

### Ajuster les paramÃ¨tres RAG

```python
# Dans api.py ou qa_cli.py
TOP_K = 6  # Plus de documents rÃ©cupÃ©rÃ©s
temperature = 0.1  # RÃ©ponses plus conservatrices
chunk_size = 1000  # Chunks plus grands
```

### Formats supportÃ©s

| Format | Extension | Loader |
|--------|-----------|---------|
| PDF | `.pdf` | PyPDFLoader |
| Texte | `.txt` | TextLoader |
| Markdown | `.md` | TextLoader |

## ğŸ› DÃ©pannage

### ProblÃ¨me : Ollama non accessible
```bash
# VÃ©rifier qu'Ollama fonctionne
ollama list
curl http://localhost:11434/api/tags
```

### ProblÃ¨me : ModÃ¨les manquants
```bash
ollama pull nomic-embed-text
ollama pull mistral:7b-instruct
```

### ProblÃ¨me : Index corrompue
```bash
# Supprimer et recrÃ©er l'index
rm -rf index/
python ingest.py
```

### ProblÃ¨me : MÃ©moire insuffisante
```python
# RÃ©duire la taille des chunks dans utils.py
chunk_size = 400
chunk_overlap = 50
```

## ğŸ“Š MÃ©triques et Monitoring

### Statistiques via CLI
```bash
python qa_cli.py
> /stats
```

### API Health Check
```bash
curl http://localhost:8000/health
```

### Monitoring Ollama
```bash
# Utilisation GPU/CPU
ollama ps
```

## ğŸ¤ Contribution

1. **Fork** le projet
2. CrÃ©er une **branche feature** (`git checkout -b feature/amelioration`)
3. **Commit** vos changements (`git commit -am 'Ajout fonctionnalitÃ©'`)
4. **Push** sur la branche (`git push origin feature/amelioration`)
5. CrÃ©er une **Pull Request**

## ğŸ”® Roadmap

- [ ] ğŸŒ Interface Web avec Streamlit/Gradio
- [ ] ğŸ“„ Support Word/PowerPoint
- [ ] ğŸ” Recherche hybride (vectorielle + full-text)
- [ ] ğŸ‘¥ Gestion multi-utilisateurs
- [ ] ğŸ“ˆ Analytics et mÃ©triques avancÃ©es
- [ ] ğŸ¨ ThÃ¨mes et personnalisation UI
- [ ] ğŸ”„ Synchronisation dossiers automatique

## ğŸ“ Changelog

### v2.0.0 (2024-08-27)
- âœ¨ Interface CLI enrichie avec Rich
- ğŸ”§ API FastAPI complÃ¨te
- ğŸ›¡ï¸ Gestion d'erreurs robuste
- ğŸ“Š Commandes statistiques
- ğŸ¯ Validation des paramÃ¨tres

### v1.0.0 (Initial)
- ğŸ—ï¸ Architecture RAG de base
- ğŸ“š Ingestion PDF/TXT
- ğŸ¤– Integration Ollama
- ğŸ’¾ Stockage Chroma

## ğŸ“„ Licence

Ce projet est sous licence **MIT**. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## â­ Support

Si ce projet vous aide, pensez Ã  lui donner une â­ !

**CrÃ©Ã© avec â¤ï¸ par ![TF-Jordan](https://github.com/TF-Jordan/ChatBot-Q-A)**

---

*QA Local - Votre assistant intelligent pour interroger vos documents en local, sans cloud, sans compromis sur la confidentialitÃ©.*
